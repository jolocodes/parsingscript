!pip install sqlparse
-------------------------
import sqlparse
-------------------------
import os
-------------------------
hive_dir = "/Users/joss/CodeProjects/hive"
hive_jobs = [
"file1_test.q"
"file2_test.q"
"file3_test.q"
]

-------------------------
hive_list = [hive_dir + "/" + job for job in hive_jobs]
hive_list
-------------------------

outputFileName = os.path.join("/Users/joss/CodeProjects/hive", "hive.csv")
print(outputFileName)
outputFile = open(outputFileName, 'w')
outputFile.write("Hive File,Type,Table Name\n")
for hive_f in hive_jobs:
    conf_step = """
      step_{step_no} = {{
        class-name = spark.pipeline.SparkLoaderStep,
        inputs = {{
          df-step-{step_no} = {{
            order = 1, sql = {{
              sql-type = local, table = step_table_{step_no},
              sql = \"\"\" {step_sql} \"\"\"
            }}, metadata = {{is-input = true, is-save = false}}
          }}
        }}
      }}, 
      """
    conf_pipeline = """
    step_{step_no},"""

    conf_pipelines = """
      pipeline {
        name = "Pipeline1"
        steps = [
    """
    conf_steps = "steps { "

    for i, sql in enumerate(open(os.path.join(qboa_dir,hive_f),"r").read().split(";")):
        sql_s = sql.strip()
        if sql_s:
            conf_steps += conf_step.format(step_no=str(i),step_sql=sql_s)
            conf_pipelines += conf_pipeline.format(step_no=str(i))
    conf_steps += "}"
    conf_pipelines += "]}\n"

    conf_f = os.path.join("/tmp",hive_f.replace(".q",".conf"))
    f = open(conf_f,"w")
    f.write(conf_pipelines + conf_steps)
    f.close()
    config = conf_f
    conf = config.parseConfigFile()
    for out in config.outputTables.keys():
        outputFile.write(hive_f+",TARGET,"+out+"\n")
    for src in config.srcTables.keys():
        outputFile.write(hive_f+",SOURCE,"+src+"\n")
outputFile.close()
